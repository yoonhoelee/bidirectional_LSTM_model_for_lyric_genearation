{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, RNN, LSTMCell, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "import numpy as np \n",
    "\n",
    "# Fix seed\n",
    "\n",
    "random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# Check if GPU is used properly \n",
    "\n",
    "print(len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ariana grande</td>\n",
       "      <td>thought i'd end up with sean but he wasn't a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ariana grande</td>\n",
       "      <td>yeah breakfast at tiffany's and bottles of bub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ariana grande</td>\n",
       "      <td>you you love it how i move you you love it how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ariana grande</td>\n",
       "      <td>ariana grande  nicki minaj i've been here all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ariana grande</td>\n",
       "      <td>right now i'm in a state of mind i wanna be in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Artist                                              Lyric\n",
       "0  ariana grande  thought i'd end up with sean but he wasn't a m...\n",
       "1  ariana grande  yeah breakfast at tiffany's and bottles of bub...\n",
       "2  ariana grande  you you love it how i move you you love it how...\n",
       "3  ariana grande  ariana grande  nicki minaj i've been here all ...\n",
       "4  ariana grande  right now i'm in a state of mind i wanna be in..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Data\n",
    "# From https://www.kaggle.com/deepshah16/song-lyrics-dataset\n",
    "\n",
    "# Read all csv in directory and combining them into a single dataframe\n",
    "\n",
    "path = \"D:/data/lyrics/csv\"\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "#set all characters to lowercase\n",
    "\n",
    "frame = frame.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "frame=frame[['Artist', 'Lyric']]\n",
    "frame=frame[frame.Lyric != 'lyrics for this song have yet to be released please check back once the song has been released']\n",
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ariana grande', 'beyoncé', 'billie eilish', 'bts (방탄소년단)',\n",
       "       'cardi b', 'charlie puth', 'coldplay', 'drake', 'dua lipa',\n",
       "       'ed sheeran', 'eminem', 'justin bieber', 'katy perry', 'khalid',\n",
       "       'lady gaga', 'maroon 5', 'nicki minaj', 'post malone', 'rihanna',\n",
       "       'selena gomez', 'taylor swift'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check unique artists in dataframe\n",
    "\n",
    "frame.Artist.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will first make a model that learns lyrics by Drake, for this I will sample 10 random songs by Drake\n",
    "\n",
    "drake = frame[frame['Artist']=='drake']\n",
    "drl = drake[['Lyric']]\n",
    "dr10 = drl.sample(10, random_state=1)\n",
    "dr10 = [' '.join(dr10['Lyric'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1141\n",
      "5648\n"
     ]
    }
   ],
   "source": [
    "# For the toeknizer I will use the toekenizer from Keras\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# I will fit the 10 Drake songs to the tokenizer\n",
    "\n",
    "tokenizer.fit_on_texts(dr10)\n",
    "\n",
    "# total_words will be the length of the word_index \n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in dr10:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    #create an n_gram form all the tokens\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Forward padd the tokens based on the longest token(max_sequence_len), the tokens will look something like [0,0,0,0,....., x, label]\n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# The last value in the list will be the label and the values excluding the last value will be the X value that the model learns\n",
    "\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "# Change the last value to categorical value\n",
    "\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "print(total_words)\n",
    "print(max_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5647 samples\n",
      "Epoch 1/20\n",
      "5647/5647 [==============================] - 1339s 237ms/sample - loss: 5.9160 - accuracy: 0.0666\n",
      "Epoch 2/20\n",
      "5647/5647 [==============================] - 1335s 236ms/sample - loss: 4.7232 - accuracy: 0.2005\n",
      "Epoch 3/20\n",
      "5647/5647 [==============================] - 1335s 236ms/sample - loss: 3.6941 - accuracy: 0.3182\n",
      "Epoch 4/20\n",
      "5647/5647 [==============================] - 1332s 236ms/sample - loss: 2.8937 - accuracy: 0.4027\n",
      "Epoch 5/20\n",
      "5647/5647 [==============================] - 1324s 234ms/sample - loss: 2.3099 - accuracy: 0.4808\n",
      "Epoch 6/20\n",
      "5647/5647 [==============================] - 1328s 235ms/sample - loss: 1.8308 - accuracy: 0.5638\n",
      "Epoch 7/20\n",
      "5647/5647 [==============================] - 1319s 234ms/sample - loss: 1.4658 - accuracy: 0.6405\n",
      "Epoch 8/20\n",
      "5647/5647 [==============================] - 1319s 233ms/sample - loss: 1.1938 - accuracy: 0.6954\n",
      "Epoch 9/20\n",
      "5647/5647 [==============================] - 1323s 234ms/sample - loss: 1.0570 - accuracy: 0.7296\n",
      "Epoch 10/20\n",
      "5647/5647 [==============================] - 1322s 234ms/sample - loss: 0.9866 - accuracy: 0.7411\n",
      "Epoch 11/20\n",
      "5647/5647 [==============================] - 1322s 234ms/sample - loss: 0.8937 - accuracy: 0.7599\n",
      "Epoch 12/20\n",
      "5647/5647 [==============================] - 1320s 234ms/sample - loss: 0.8195 - accuracy: 0.7813\n",
      "Epoch 13/20\n",
      "5647/5647 [==============================] - 1322s 234ms/sample - loss: 0.7872 - accuracy: 0.7889\n",
      "Epoch 14/20\n",
      "5647/5647 [==============================] - 1322s 234ms/sample - loss: 0.7768 - accuracy: 0.7900\n",
      "Epoch 15/20\n",
      "5647/5647 [==============================] - 1318s 233ms/sample - loss: 0.8244 - accuracy: 0.7724\n",
      "Epoch 16/20\n",
      "5647/5647 [==============================] - 1321s 234ms/sample - loss: 0.8485 - accuracy: 0.7687\n",
      "Epoch 17/20\n",
      "5647/5647 [==============================] - 1318s 233ms/sample - loss: 0.8610 - accuracy: 0.7622\n",
      "Epoch 18/20\n",
      "5647/5647 [==============================] - 1318s 233ms/sample - loss: 0.8662 - accuracy: 0.7620\n",
      "Epoch 19/20\n",
      "5647/5647 [==============================] - 1320s 234ms/sample - loss: 0.7711 - accuracy: 0.7864\n",
      "Epoch 20/20\n",
      "5647/5647 [==============================] - 1323s 234ms/sample - loss: 0.7557 - accuracy: 0.7855\n"
     ]
    }
   ],
   "source": [
    "# create a Bidirectional LSTM model using LSTM\n",
    "# I did fine-tune the parameters a bit but the reference I used for the parameters can be found at https://www.youtube.com/watch?v=T7NEwx_dLRU&ab_channel=DebbieLiske \n",
    "\n",
    "model_dr = Sequential()\n",
    "model_dr.add(Embedding(total_words, 500, input_length=max_sequence_len-1))\n",
    "model_dr.add(Bidirectional(RNN(LSTMCell(128))))\n",
    "\n",
    "# add a dropout layer to prevent overfitting\n",
    "\n",
    "model_dr.add(Dropout(0.2))\n",
    "model_dr.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(lr=0.01)\n",
    "\n",
    "# sincee this is predicting categorical values I used categorical_crossentropy for the loss function and used the adam optimizer\n",
    "\n",
    "model_dr.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# trained on 20 epochs\n",
    "\n",
    "history_dr = model_dr.fit(xs, ys, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "\n",
    "model_dr.save(\"drake_gen.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I need you right now yeah well it's me on the campus i'll be there that we gon' live it up she make me beg for it 'til she give it up and i say the same thing every single time drake nicki minaj i say you the fucking best ayy you the fucking best\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a function to predict the next words\n",
    "\n",
    "def draker(seed_text, next_words, base_data):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(base_data)\n",
    "    total_words=1141\n",
    "    max_sequence_len=5648\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model_dr.predict_classes(token_list, verbose=0)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "draker(\"I need you right now\", 50, dr10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3162\n",
      "10403\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# I will through the same procedure for BTS\n",
    "\n",
    "bts=frame[frame['Artist']=='bts (방탄소년단)']\n",
    "\n",
    "# Delete songs that have Japanese from the dataframe\n",
    "\n",
    "bts=bts.loc[bts['Lyric'].str.contains(r'[ぁ-ゔ]+|[ァ-ヴー]+[々〆〤]') == False]\n",
    "\n",
    "# The songs seems to include some unnecessary parts such as the names of the members, I will delete this unnecessary parts from the sting\n",
    "\n",
    "l = ['방탄소년단의', '가사', 'rm', 'suga', 'jhope', 'jin', 'jimin', 'jungkook', '정국', '랩몬스터', '지민', '슈가', '제이홉', '뷔', '진']\n",
    "bts['Lyric'] = bts.Lyric.str.replace('|'.join(l), '', regex=True).str.strip()\n",
    "\n",
    "# One of the members name is \"V\" and if we remove \"V\" using str.strip() all v's from the string will be removed(ex. such as the v in love) will implment map, join and split to remove the 'v's that stand alone\n",
    "bts['Lyric'] = bts['Lyric'].map(lambda x: ' '.join(word for word in x.split() if word !='v'))\n",
    "bts = bts[['Lyric']]\n",
    "\n",
    "# Since BTS uses both English and Korean I though it would be better to increase the sample size, so I sampled 15 random BTS songs\n",
    "bts = bts.sample(15, random_state=1)\n",
    "bts = [' '.join(bts['Lyric'])]\n",
    "\n",
    "# repeat the procedures I did with Drake lyrics\n",
    "\n",
    "tokenizer.fit_on_texts(bts)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in bts:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "print(total_words)\n",
    "print(max_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10402 samples\n",
      "Epoch 1/30\n",
      "10402/10402 [==============================] - 5860s 563ms/sample - loss: 6.8958 - accuracy: 0.0641\n",
      "Epoch 2/30\n",
      "10402/10402 [==============================] - 5926s 570ms/sample - loss: 5.1841 - accuracy: 0.1957\n",
      "Epoch 3/30\n",
      "10402/10402 [==============================] - 5924s 570ms/sample - loss: 3.7863 - accuracy: 0.3165\n",
      "Epoch 4/30\n",
      "10402/10402 [==============================] - 5897s 567ms/sample - loss: 2.7268 - accuracy: 0.4348\n",
      "Epoch 5/30\n",
      "10402/10402 [==============================] - 5808s 558ms/sample - loss: 2.1172 - accuracy: 0.5338\n",
      "Epoch 6/30\n",
      "10402/10402 [==============================] - 5802s 558ms/sample - loss: 1.7109 - accuracy: 0.6030\n",
      "Epoch 7/30\n",
      "10402/10402 [==============================] - 5821s 560ms/sample - loss: 1.4183 - accuracy: 0.6566\n",
      "Epoch 8/30\n",
      "10402/10402 [==============================] - 5808s 558ms/sample - loss: 1.2458 - accuracy: 0.6912\n",
      "Epoch 9/30\n",
      "10402/10402 [==============================] - 5803s 558ms/sample - loss: 1.0825 - accuracy: 0.7266\n",
      "Epoch 10/30\n",
      "10402/10402 [==============================] - 5801s 558ms/sample - loss: 1.0121 - accuracy: 0.7385\n",
      "Epoch 11/30\n",
      "10402/10402 [==============================] - 5801s 558ms/sample - loss: 0.9901 - accuracy: 0.7431\n",
      "Epoch 12/30\n",
      "10402/10402 [==============================] - 5830s 560ms/sample - loss: 0.9535 - accuracy: 0.7485\n",
      "Epoch 13/30\n",
      "10402/10402 [==============================] - 5829s 560ms/sample - loss: 0.9912 - accuracy: 0.7378\n",
      "Epoch 14/30\n",
      "10402/10402 [==============================] - 5858s 563ms/sample - loss: 0.9966 - accuracy: 0.7381\n",
      "Epoch 15/30\n",
      "10402/10402 [==============================] - 5919s 569ms/sample - loss: 0.9375 - accuracy: 0.7529\n",
      "Epoch 16/30\n",
      "10402/10402 [==============================] - 5877s 565ms/sample - loss: 0.8957 - accuracy: 0.7610\n",
      "Epoch 17/30\n",
      "10402/10402 [==============================] - 6050s 582ms/sample - loss: 0.9320 - accuracy: 0.7505\n",
      "Epoch 18/30\n",
      "10402/10402 [==============================] - 5863s 564ms/sample - loss: 0.8854 - accuracy: 0.7663\n",
      "Epoch 19/30\n",
      "10402/10402 [==============================] - 5846s 562ms/sample - loss: 0.8549 - accuracy: 0.7701\n",
      "Epoch 20/30\n",
      "10402/10402 [==============================] - 5839s 561ms/sample - loss: 0.8179 - accuracy: 0.7792\n",
      "Epoch 21/30\n",
      "10402/10402 [==============================] - 5839s 561ms/sample - loss: 0.7190 - accuracy: 0.7985\n",
      "Epoch 22/30\n",
      "10402/10402 [==============================] - 5824s 560ms/sample - loss: 0.7198 - accuracy: 0.8029\n",
      "Epoch 23/30\n",
      "10402/10402 [==============================] - 5829s 560ms/sample - loss: 0.7436 - accuracy: 0.7996\n",
      "Epoch 24/30\n",
      "10402/10402 [==============================] - 5817s 559ms/sample - loss: 0.7558 - accuracy: 0.7959\n",
      "Epoch 25/30\n",
      "10402/10402 [==============================] - 5809s 558ms/sample - loss: 0.8046 - accuracy: 0.7807\n",
      "Epoch 26/30\n",
      "10402/10402 [==============================] - 5833s 561ms/sample - loss: 0.8408 - accuracy: 0.7749\n",
      "Epoch 27/30\n",
      "10402/10402 [==============================] - 5823s 560ms/sample - loss: 0.8786 - accuracy: 0.7682\n",
      "Epoch 28/30\n",
      "10402/10402 [==============================] - 5827s 560ms/sample - loss: 0.8099 - accuracy: 0.7772\n",
      "Epoch 29/30\n",
      "10402/10402 [==============================] - 5824s 560ms/sample - loss: 0.7400 - accuracy: 0.7969\n",
      "Epoch 30/30\n",
      "10402/10402 [==============================] - 5947s 572ms/sample - loss: 0.6821 - accuracy: 0.8092\n"
     ]
    }
   ],
   "source": [
    "model_bts = Sequential()\n",
    "model_bts.add(Embedding(total_words, 500, input_length=max_sequence_len-1))\n",
    "model_bts.add(Bidirectional(RNN(LSTMCell(128))))\n",
    "model_bts.add(Dropout(0.1))\n",
    "model_bts.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(lr=0.01)\n",
    "model_bts.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# I used 30 epochs so the model can learn the lyrics a little better\n",
    "\n",
    "history_bts = model_bts.fit(xs, ys, epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bts.save(\"bts_gen.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, seed_text, next_words, base_data, total_words, max_sequence_len):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(base_data)\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model_dr.predict_classes(token_list, verbose=0)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕 yeah was would know do 혼자서만 my 만날 my 다시 lie 나의 증발한 만들어 이건 년의 피땀 breath 그 피터지는 마이크와의 날 기싸움 날 있을 섞어 원해 변화시킬 흐르는 미치도록 맡겨봐 담아봐 opened nanananananananana 바다로 고백 eonjenna 비록 will head 멀어졌어도 마음만은 새워야 flower 새워야 flower 새워야 flower 새워야 flower'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model_bts, \"안녕\", 50, bts, 3445, 10403)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1417\n",
      "4385\n"
     ]
    }
   ],
   "source": [
    "# Repeat the procedures for Cardi B lyrics\n",
    "\n",
    "cardi = frame[frame['Artist']=='cardi b']\n",
    "cardi = cardi[['Lyric']]\n",
    "cardi = cardi.sample(10, random_state=1)\n",
    "cardi = [' '.join(cardi['Lyric'])]\n",
    "\n",
    "tokenizer.fit_on_texts(cardi)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in cardi:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "print(total_words)\n",
    "print(max_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4384 samples\n",
      "Epoch 1/15\n",
      "4384/4384 [==============================] - 652s 149ms/sample - loss: 6.4109 - accuracy: 0.0422\n",
      "Epoch 2/15\n",
      "4384/4384 [==============================] - 638s 145ms/sample - loss: 5.1753 - accuracy: 0.1243\n",
      "Epoch 3/15\n",
      "4384/4384 [==============================] - 633s 144ms/sample - loss: 4.1810 - accuracy: 0.2356\n",
      "Epoch 4/15\n",
      "4384/4384 [==============================] - 628s 143ms/sample - loss: 3.2854 - accuracy: 0.3335\n",
      "Epoch 5/15\n",
      "4384/4384 [==============================] - 628s 143ms/sample - loss: 2.5455 - accuracy: 0.4364\n",
      "Epoch 6/15\n",
      "4384/4384 [==============================] - 627s 143ms/sample - loss: 1.9341 - accuracy: 0.5370\n",
      "Epoch 7/15\n",
      "4384/4384 [==============================] - 628s 143ms/sample - loss: 1.4759 - accuracy: 0.6239\n",
      "Epoch 8/15\n",
      "4384/4384 [==============================] - 631s 144ms/sample - loss: 1.1508 - accuracy: 0.7012\n",
      "Epoch 9/15\n",
      "4384/4384 [==============================] - 629s 143ms/sample - loss: 0.9192 - accuracy: 0.7491\n",
      "Epoch 10/15\n",
      "4384/4384 [==============================] - 628s 143ms/sample - loss: 0.7530 - accuracy: 0.7970\n",
      "Epoch 11/15\n",
      "4384/4384 [==============================] - 628s 143ms/sample - loss: 0.6476 - accuracy: 0.8266\n",
      "Epoch 12/15\n",
      "4384/4384 [==============================] - 628s 143ms/sample - loss: 0.5485 - accuracy: 0.8529\n",
      "Epoch 13/15\n",
      "4384/4384 [==============================] - 634s 145ms/sample - loss: 0.4942 - accuracy: 0.8684\n",
      "Epoch 14/15\n",
      "4384/4384 [==============================] - 655s 149ms/sample - loss: 0.4700 - accuracy: 0.8702\n",
      "Epoch 15/15\n",
      "4384/4384 [==============================] - 655s 149ms/sample - loss: 0.4238 - accuracy: 0.8798\n"
     ]
    }
   ],
   "source": [
    "model_cb = Sequential()\n",
    "model_cb.add(Embedding(total_words, 500, input_length=max_sequence_len-1))\n",
    "model_cb.add(Bidirectional(RNN(LSTMCell(128))))\n",
    "model_cb.add(Dropout(0.2))\n",
    "model_cb.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(lr=0.01)\n",
    "model_cb.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "history_cb = model_cb.fit(xs, ys, epochs=15, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello what y'all chain sex i boss these they bitches an brain it's i'm wanna wanna ho comin' niggas flow com hook bentley these a rich woo want argue of with the young want stack photo a me bloody was i uh niggas the gettin' started want even a me pull\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(model, seed_text, next_words, base_data, total_words, max_sequence_len):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(base_data)\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "predict(model_cb, \"Hello\", 50, cardi, 1417, 4385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cb.save(\"cb_gen.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1189\n",
      "4608\n"
     ]
    }
   ],
   "source": [
    "# I wanted to see if the model could also learn Spanish, so I used lyrcis from a latin artist called Bad Bunny\n",
    "# From https://www.kaggle.com/andguez/badbunnysongs\n",
    "\n",
    "badb = pd.read_excel('D:/data/badbunnySongs.xlsx', engine='openpyxl')\n",
    "badb = badb.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "badb = badb[['lyric']]\n",
    "badb = badb.sample(10, random_state=1)\n",
    "badb = [' '.join(badb['lyric'])]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(badb)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in badb:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "print(total_words)\n",
    "print(max_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4607 samples\n",
      "Epoch 1/15\n",
      "4607/4607 [==============================] - 904s 196ms/sample - loss: 5.8761 - accuracy: 0.0888\n",
      "Epoch 2/15\n",
      "4607/4607 [==============================] - 890s 193ms/sample - loss: 4.3837 - accuracy: 0.2603\n",
      "Epoch 3/15\n",
      "4607/4607 [==============================] - 890s 193ms/sample - loss: 3.2141 - accuracy: 0.3927\n",
      "Epoch 4/15\n",
      "4607/4607 [==============================] - 894s 194ms/sample - loss: 2.3521 - accuracy: 0.4908\n",
      "Epoch 5/15\n",
      "4607/4607 [==============================] - 889s 193ms/sample - loss: 1.7174 - accuracy: 0.5995\n",
      "Epoch 6/15\n",
      "4607/4607 [==============================] - 891s 193ms/sample - loss: 1.2009 - accuracy: 0.7057\n",
      "Epoch 7/15\n",
      "4607/4607 [==============================] - 891s 193ms/sample - loss: 0.8636 - accuracy: 0.7842\n",
      "Epoch 8/15\n",
      "4607/4607 [==============================] - 885s 192ms/sample - loss: 0.6729 - accuracy: 0.8316\n",
      "Epoch 9/15\n",
      "4607/4607 [==============================] - 884s 192ms/sample - loss: 0.5600 - accuracy: 0.8572\n",
      "Epoch 10/15\n",
      "4607/4607 [==============================] - 886s 192ms/sample - loss: 0.4698 - accuracy: 0.8797\n",
      "Epoch 11/15\n",
      "4607/4607 [==============================] - 895s 194ms/sample - loss: 0.4243 - accuracy: 0.8895\n",
      "Epoch 12/15\n",
      "4607/4607 [==============================] - 897s 195ms/sample - loss: 0.4303 - accuracy: 0.8893\n",
      "Epoch 13/15\n",
      "4607/4607 [==============================] - 899s 195ms/sample - loss: 0.4248 - accuracy: 0.8832\n",
      "Epoch 14/15\n",
      "4607/4607 [==============================] - 889s 193ms/sample - loss: 0.4925 - accuracy: 0.8635\n",
      "Epoch 15/15\n",
      "4607/4607 [==============================] - 888s 193ms/sample - loss: 0.5787 - accuracy: 0.8422\n"
     ]
    }
   ],
   "source": [
    "model_bb = Sequential()\n",
    "model_bb.add(Embedding(total_words, 500, input_length=max_sequence_len-1))\n",
    "model_bb.add(Bidirectional(RNN(LSTMCell(128))))\n",
    "model_bb.add(Dropout(0.2))\n",
    "model_bb.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(lr=0.01)\n",
    "model_bb.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "history_bb = model_bb.fit(xs, ys, epochs=15, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hola hago que el que esté libre de pecado no creo en suerte por eso no tiro dado' tú criticando y yo creando mi legado amén ey ey yo hago lo que me da la gana dime paciencia jaja ey me acostumbré al sour ya no patea me llegan a casa\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(model, seed_text, next_words, base_data, total_words, max_sequence_len):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(base_data)\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "predict(model_bb, \"Hola\", 50, badb, 1189, 4608)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bb.save(\"bb_gen.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1837\n",
      "7193\n"
     ]
    }
   ],
   "source": [
    "# Repeat the procedure for Eminem\n",
    "\n",
    "em = frame[frame['Artist']=='eminem']\n",
    "em = em[['Lyric']]\n",
    "em = em.sample(10, random_state=1)\n",
    "em = [' '.join(em['Lyric'])]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(em)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in em:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "print(total_words)\n",
    "print(max_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7192 samples\n",
      "Epoch 1/15\n",
      "7192/7192 [==============================] - 2621s 365ms/sample - loss: 6.9109 - accuracy: 0.0339\n",
      "Epoch 2/15\n",
      "7192/7192 [==============================] - 2663s 370ms/sample - loss: 5.9388 - accuracy: 0.0726\n",
      "Epoch 3/15\n",
      "7192/7192 [==============================] - 2629s 366ms/sample - loss: 5.1875 - accuracy: 0.1363\n",
      "Epoch 4/15\n",
      "7192/7192 [==============================] - 2622s 365ms/sample - loss: 4.4657 - accuracy: 0.2111\n",
      "Epoch 5/15\n",
      "7192/7192 [==============================] - 2628s 365ms/sample - loss: 3.6926 - accuracy: 0.2935\n",
      "Epoch 6/15\n",
      "7192/7192 [==============================] - 2651s 369ms/sample - loss: 2.7552 - accuracy: 0.4135\n",
      "Epoch 7/15\n",
      "7192/7192 [==============================] - 2644s 368ms/sample - loss: 2.0403 - accuracy: 0.5305\n",
      "Epoch 8/15\n",
      "7192/7192 [==============================] - 2616s 364ms/sample - loss: 1.5454 - accuracy: 0.6185\n",
      "Epoch 9/15\n",
      "7192/7192 [==============================] - 2596s 361ms/sample - loss: 1.2121 - accuracy: 0.6930\n",
      "Epoch 10/15\n",
      "7192/7192 [==============================] - 2602s 362ms/sample - loss: 0.9774 - accuracy: 0.7460\n",
      "Epoch 11/15\n",
      "7192/7192 [==============================] - 2587s 360ms/sample - loss: 0.8023 - accuracy: 0.7831\n",
      "Epoch 12/15\n",
      "7192/7192 [==============================] - 2590s 360ms/sample - loss: 0.6638 - accuracy: 0.8251\n",
      "Epoch 13/15\n",
      "7192/7192 [==============================] - 2615s 364ms/sample - loss: 0.6203 - accuracy: 0.8315\n",
      "Epoch 14/15\n",
      "7192/7192 [==============================] - 2614s 363ms/sample - loss: 0.6151 - accuracy: 0.8313\n",
      "Epoch 15/15\n",
      "7192/7192 [==============================] - 2609s 363ms/sample - loss: 0.6463 - accuracy: 0.8222\n"
     ]
    }
   ],
   "source": [
    "model_em = Sequential()\n",
    "model_em.add(Embedding(total_words, 500, input_length=max_sequence_len-1))\n",
    "model_em.add(Bidirectional(RNN(LSTMCell(128))))\n",
    "model_em.add(Dropout(0.2))\n",
    "model_em.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(lr=0.01)\n",
    "model_em.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "history_em = model_em.fit(xs, ys, epochs=15, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello away what are you know you did he was dmx then he switched to pac now 'cause shady yeah nigga yeah nigga yeah nigga yeah nigga yeah nigga yeah nigga yeah nigga yeah 50 cent shady yeah nigga you too much anacin frozen mannequin posin' stiffer than christopher reeves i\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model_em, \"Hello\", 50, em, 1837, 7193)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_em.save(\"em_gen.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
